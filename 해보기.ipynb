{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#단어를 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[['i', 'am', 'programming', 'python'], ['economics', 'is', 'a', 'study', 'of', 'money'], ['python', 'is', 'easy'], ['i', 'like', 'money'], ['i', 'like', 'economics', 'and', 'study'], ['i', 'like', 'easy', 'programming', 'economics', 'is', 'money']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "\n",
    "docs = [\"I am programming Python\",\n",
    "       \"Economics is a study of money\",\n",
    "       \"Python is easy\",\n",
    "       \"I like money\",\n",
    "       \"i like Economics and study\",\n",
    "       \"I like easy programming Economics is money\"]\n",
    "print(type(docs))\n",
    "\n",
    "        \n",
    "data = [[word.lower() for word in word_tokenize(doc)] for doc in docs]\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어에서 stopwords제거-문서별로 단어가 추출되어있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['programming', 'python'], ['economics', 'study', 'money'], ['python', 'easy'], ['like', 'money'], ['like', 'economics', 'study'], ['like', 'easy', 'programming', 'economics', 'money']]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def remove_stopwords(docs):\n",
    "    return [[word for word in doc if word not in STOP_WORDS] for doc in docs]\n",
    "\n",
    "data = remove_stopwords(data)\n",
    "print (data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이젠 문서들을 한덩어리로 보고 전체에서 단어를 추출한다 / 단어에 숫자메기고(dictionary) 그걸 컴이 알아듣게 숫자로 바꿈(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['programming', 'python', 'economics', 'money', 'study', 'easy', 'like']\n",
      "[(0, 'programming'), (1, 'python'), (2, 'economics'), (3, 'money'), (4, 'study'), (5, 'easy'), (6, 'like')]\n",
      "[[(0, 1), (1, 1)], [(2, 1), (3, 1), (4, 1)], [(1, 1), (5, 1)], [(3, 1), (6, 1)], [(2, 1), (4, 1), (6, 1)], [(0, 1), (2, 1), (3, 1), (5, 1), (6, 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "#숫자 부여하기\n",
    "id2word = corpora.Dictionary(data)\n",
    "#이런식으로 부여되어있다 사람한테 보여줌\n",
    "dic = [id2word[i] for i in range (len(id2word))]\n",
    "\n",
    "print(dic)\n",
    "print([(i, id2word[i]) for i in range(len(id2word))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##컴이 알아듣게 숫자로 바꿈-이건 아직 문서들이 나눠져있음\n",
    "corpus = [id2word.doc2bow(doc) for doc in data]\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA_model-앞서 만든 corpus와 id2word를 가지고 컴퓨터가 토픽을 2개(내가정함)생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=7, num_topics=2, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10, per_word_topics=True)\n",
    "print(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.236*\"economics\" + 0.214*\"money\" + 0.213*\"like\" + 0.201*\"study\" + '\n",
      "  '0.046*\"python\" + 0.045*\"programming\" + 0.044*\"easy\"'),\n",
      " (1,\n",
      "  '0.210*\"easy\" + 0.209*\"programming\" + 0.208*\"python\" + 0.117*\"like\" + '\n",
      "  '0.117*\"money\" + 0.094*\"economics\" + 0.044*\"study\"')]\n"
     ]
    }
   ],
   "source": [
    "#토픽 2개의 구성은 각각 이렇다\n",
    "from pprint import pprint\n",
    "pprint(lda_model.show_topics(num_topics=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성한 토픽 2개와 7개의 문서가 각각 얼만큼 관계있는지 알려줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.171145,0.828855, I am programming Python\n",
      "0.863429,0.136571, Economics is a study of money\n",
      "0.171028,0.828972, Python is easy\n",
      "0.802594,0.197406, I like money\n",
      "0.863308,0.136692, i like Economics and study\n",
      "0.422543,0.577457, I like easy programming Economics is money\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(corpus)):\n",
    "    topic0, topic1 = lda_model.get_document_topics(corpus[i])\n",
    "    print('{0:f},{1:f}, {2:s}'.format(topic0[1], topic1[1], docs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아까 전체문서에서 뽑은 단어 7개(dic)가 각 토픽과 얼만큼 관계있는지 보여줌\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015414,0.175820, programming\n",
      "0.016245,0.174661, python\n",
      "0.204660,0.057997, economics\n",
      "0.182193,0.080850, money\n",
      "0.169366,0.013435, study\n",
      "0.014716,0.176808, easy\n",
      "0.181594,0.081467, like\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dic)):\n",
    "    topic0, topic1 =lda_model.get_term_topics(dic[i])\n",
    "    print('{0:f},{1:f}, {2:s}'.format(topic0[1], topic1[1], dic[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이거뭐냐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 0.17114735), (1, 0.82885265)],\n",
       " [(0, [1]), (1, [1])],\n",
       " [(0, [(1, 0.9934996)]), (1, [(1, 0.9931066)])])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.get_document_topics(corpus[0], per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 단어가 2개의 토픽과 얼만큼 관련있는지. Programming이란 단어는 토픽 1과 0.993만큼 관련 있음 / Economics란 단어는 토픽 0과 토픽 1 둘 다와 관련이 있다. 같은단어라도 서로 다른 문서에 속해 있다면 주어진 2개의 토픽과 연관률이 다를 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['programming', 'python'], ['economics', 'study', 'money'], ['python', 'easy'], ['like', 'money'], ['like', 'economics', 'study'], ['like', 'easy', 'programming', 'economics', 'money']]\n",
      "programming [(1, 0.9935011)]\n",
      "python [(1, 0.9931083)]\n",
      "economics [(0, 0.9837064), (1, 0.016293552)]\n",
      "study [(0, 0.9747184), (1, 0.025281657)]\n",
      "money [(0, 0.9953848)]\n",
      "python [(1, 0.9931186)]\n",
      "easy [(1, 0.9938371)]\n",
      "like [(0, 0.9543172), (1, 0.045682725)]\n",
      "money [(0, 0.9538398), (1, 0.046160113)]\n",
      "like [(0, 0.9836767), (1, 0.016323367)]\n",
      "economics [(0, 0.99537617)]\n",
      "study [(0, 0.9744021), (1, 0.025597872)]\n",
      "like [(0, 0.05704304), (1, 0.942957)]\n",
      "easy [(0, 0.7088725), (1, 0.29112744)]\n",
      "programming [(0, 0.6085987), (1, 0.39140126)]\n",
      "economics [(0, 0.054312736), (1, 0.94568723)]\n",
      "money [(0, 0.6060002), (1, 0.39399984)]\n"
     ]
    }
   ],
   "source": [
    "print(data) #data는 문서별로 나눠져있던 것\n",
    "\n",
    "for i in range(len(data)):\n",
    "    word_topics = lda_model.get_document_topics(corpus[i], per_word_topics=True)\n",
    "    for j in range(len(data[i])):\n",
    "        topic = word_topics[2][j][1]\n",
    "        print(data[i][j], topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
